# Semi-Supervised Paper Record
| Number | Paper Name|  Published | Motivation |
| :-: | :---: | :---: | :-- |
| 4 | [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929) | ICLR<br>2021 | VIT：Vision Transformer图片输入打成patch，每个patch相当于一个单词，Transformer架构不变，打通了NLP和CV鸿沟 |
| 3 | [Deep residual learning for image recognition](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) | CVPR<br>2016 | ResNet：解决网络层数变深，性能下降的问题，学习残差；理论解释少，可以从梯度角度解释为什么训练速度快 |
| 2 | [ImageNet classification with deep convolutional neural networks](https://dl.acm.org/doi/abs/10.1145/3065386) | NIPS<br>2012 | AlexNet五层卷积+三层全连接，文章双卡训练，模型切开细节见原文 |
| 1 | [Attention Is All You Need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) | NIPS<br>2017 | Transformer：Self-Attention替换之前的CNN、RNN，优缺点及具体见纸质论文记录 |
